---
title: "Web Experiment Analysis"
author: "Bodo Winter"
date: "7/30/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Preliminaries

Load packages:

```{r, message = FALSE}
library(tidyverse) # for data wrangling
library(ade4) # for mantel tests
```

Load data:

```{r, message = FALSE, warning = FALSE}
web <- read_csv('../data/online_cleaned.csv')
lang_info <- read_csv('../data/language_info.csv')
```

Load ggplot2 theme:

```{r}
source('theme_timo.R')
```

## Demographics and participant N to report

How many speakers?

```{r}
length(unique(web$ID))
```

How many men and women?

```{r}
table(filter(web, !duplicated(ID))$Sex)
```

What is the average age and age range?

```{r}
# Average age:

mean(filter(web, !duplicated(ID))$Age, na.rm = TRUE)

# Age range:

range(filter(web, !duplicated(ID))$Age, na.rm = TRUE)
```

How many languages?

```{r}
length(unique(web$Language))
```

Minus 1 for English UK == English US when we look at the language level.

How many families?

```{r}
length(unique(web$Genus))
```

How many speakers per language?

```{r}
sort(table(filter(web, !duplicated(ID))$Language))
```

## Create a table with all info for the paper:

First, get the counts:

```{r}
web_counts <- sort(table(filter(web, !duplicated(ID))$Language))

web_counts <- tibble(Language = names(web_counts),
                     N = as.vector(web_counts))
```

Then add the language name:

```{r}
web_counts <- left_join(web_counts, select(lang_info, Language, Name, Family))
```

Add descriptive accuracies:

```{r}
web_accs <- web %>% group_by(Language) %>% 
  summarize(ACC = mean(ACC))

web_counts <- left_join(web_accs, web_counts)
```

Sort by language family:

```{r}
web_counts <- arrange(web_counts, Family, Name)
```

Round accuracy:

```{r}
web_counts <- mutate(web_counts,
                     ACC = round(ACC, 2))
```

Sort columns:

```{r}
web_counts <- select(web_counts,
       Name, Family, N, ACC)
```

Write this to a table:

```{r}
write_csv(web_counts,
          '../data/web_experiment_counts.csv')
```

Report how many speakers we have per language on average:

```{r}
mean(web_counts$N)
```

## Descriptive statistics

Check overall accuracy:

```{r}
str_c(round(mean(web$ACC), 2) * 100, '%')
```

Check accuracy by language:

```{r}
lang_avgs <- web %>% group_by(Name, Family) %>% 
  summarize(ACC = mean(ACC)) %>% 
  arrange(desc(ACC))

# Check:

lang_avgs %>% print(n = Inf)
```

Check accuracy by language family:

```{r}
family_avgs <- web %>% group_by(Genus) %>% 
  summarize(ACC = mean(ACC)) %>% 
  arrange(desc(ACC))

# Check:

family_avgs %>% print(n = Inf)
```

Check accuracy by language area:

```{r}
area_avgs <- web %>% group_by(Autotyp_Area) %>% 
  summarize(ACC = mean(ACC)) %>% 
  arrange(desc(ACC))

# Check:

area_avgs %>% print(n = Inf)
```

Check accuracy by item:

```{r}
item_avgs <- web %>% group_by(Meaning, category, subcategory) %>% 
  summarize(ACC = mean(ACC)) %>% 
  arrange(desc(ACC))

# Check:

item_avgs %>% print(n = Inf)
```

Save this for a table that makes Marcus happy:

```{r}
write_csv(mutate(item_avgs, ACC = round(ACC, 2)),
          '../data/by_item_table_web.csv')
```

Check accuracy by team:

```{r}
web %>% group_by(Team) %>% 
  summarize(ACC = mean(ACC)) %>% 
  arrange(desc(ACC)) %>% 
  print(n = Inf)
```

Check accuracy by meaning categories:

```{r}
web %>% group_by(category) %>% 
  summarize(ACC = mean(ACC)) %>% 
  arrange(desc(ACC)) %>% 
  print(n = Inf)
```

Check accuracy by meaning subcategories:

```{r}
web %>% group_by(subcategory) %>% 
  summarize(ACC = mean(ACC)) %>% 
  arrange(desc(ACC)) %>% 
  print(n = Inf)
```

Check cultural similarity:

```{r}
germanics <- c('English UK', 'English US', 'German', 'Swedish', 'Danish')
other_IE <- c('Albanian', 'Spanish', 'Farsi', 'French', 'Greek', 'Italian',
              'Polish', 'Portuguese', 'Romanian', 'Russian')
web <- mutate(web,
              CultureSim = ifelse(Name %in% germanics,
                                  'Germanic', Name),
              CultureSim = ifelse(CultureSim %in% other_IE,
                                  'IE', CultureSim),
              CultureSim = ifelse(Family != 'IE',
                                  'non-IE', CultureSim))
```

Check accuracy by this variable:

```{r}
web %>% group_by(CultureSim) %>% 
  summarize(ACC = mean(ACC))
```



## Age / gender:

Check accuracy by gender:

```{r}
# By-speaker averages:

speaker_avgs <- web %>% group_by(ID, Language, Sex, Age) %>% 
  summarize(ACC = mean(ACC)) %>% 
  arrange(desc(ACC)) %>% 
  print(n = 10)

# Average:

speaker_avgs %>% group_by(Sex) %>% 
  summarize(ACC = mean(ACC))
```

How any speakers above chance?

```{r}
sum(speaker_avgs$ACC > 1/6) / nrow(speaker_avgs)
```

All speakers above chance!

Check accuracy by age:

```{r}
# Correlation value:

with(speaker_avgs, cor(Age, ACC, method = 'spearman'))
```

No strong correlation. Check age across languages, from oldest to youngest:

```{r}
speaker_avgs %>% group_by(Language) %>% 
  summarize(Age = mean(Age)) %>% 
  arrange(desc(Age)) %>% 
  print(n = Inf)
```

How does accuracy depend on playbacks?

```{r}
web %>% group_by(Rep) %>% 
  summarize(ACC = mean(ACC)) %>% 
  print(n = Inf)
```

If at all accuracy decreases.


## Marcus's descriptive statistics: how many meanings per language above chance?

A simple way to summarize this data is to check how many meanings are guessed above chance per language.

First, let's get a vector with the language names which we'll use for looping:

```{r}
languages <- unique(web$Language)

# Check:

languages
```

Put this into a tibble:

```{r}
meanings_per_language <- tibble(Language = languages)
meanings_per_language$N_over_chance <- NA

# Check:

meanings_per_language
```

Loop through languages and get the N of items that have > 1/12 accuracy:

```{r}
for (i in seq_along(languages)) {
  this_lang <- languages[i]
  these_avgs <- filter(web, Language == this_lang) %>%
    group_by(Meaning) %>% 
    summarize(ACC = mean(ACC))
  meanings_per_language[i, ]$N_over_chance <- sum(these_avgs$ACC > 1/6)
}
```

Check the results:

```{r}
meanings_per_language %>% print(n = Inf)
```

The easiest thing is to compute how many had N meanings correct:

```{r}
meanings_per_language %>% count(N_over_chance) %>% 
  mutate(proportion = n / nrow(meanings_per_language))
```

72% of all languages had all meanings over chance. All languages had at least 28 meanings over chance!

## Check influence of L2:

How many speakers are there per L2 category?

```{r}
L2_counts <- with(web, table(ID, L2_simplified))
labels <- colnames(L2_counts)
L2_counts <- table(apply(L2_counts, MARGIN = 1,
                         FUN = function(x) labels[which(x != 0)]))
```

Counts:

```{r}
L2_counts
```

Proportions:

```{r}
round(L2_counts / sum(L2_counts), 2)
```

Check accuracy by the four different categories:

```{r}
web %>% group_by(L2_simplified) %>% 
  summarize(ACC = mean(ACC)) %>% 
  mutate(ACC = round(ACC, 2))
```

## Confusion matrices:

Check confusion, first for English:

```{r}
EN <- filter(web, Language == 'EN')
EN_tab <- table(EN$Meaning, EN$Choice)
EN_tab
```

Looks good. We want to do this for all of the six languages an then correlate these tables with the same tables from the other languages.

We can re-use the "languages" vector from above for this.

Create an empty 6 * 6 matrix (filled with NAs) to be filled with all the languages:

```{r}
M <- rep(NA, length(languages) * length(languages))
M <- matrix(M, nrow = length(languages))

# Append row and column names:

rownames(M) <- languages
colnames(M) <- languages

# Check:

M
```

Create two copies of this matrix, one for the correlation coefficients, another one for the p-values:

```{r}
r_vals <- M
p_vals <- M
```

For this it's good to have everything as factor so that all matrices are the same (since they all work with the same factors: not all meanings were chosen in all languages).

And for this it makes sense to get rid of the choices that were "clapping". These are incorrect in the main analysis below, but they cause problems here since "clapping" isn't attested in the meanings at this stage in the analysis (since clapping sounds have been excluded) and thus leads to non-square matrices.

```{r}
# Get rid of clapping:

web2 <- filter(web,
               Choice != 'clapping')

# Factor code:

web2 <- mutate(web2,
               Meaning = factor(Meaning),
               Choice = factor(Choice))
```

Loop through languages and create distance matrices:

```{r}
for (i in seq_along(languages)) {
  # Cross-tabulation:
  
  this_tab <- with(filter(web2, Language == languages[i]), table(Meaning, Choice))
  
  # Get rid of the clapping from the choices, otherwise we have non-square matrices (since clapping isn't in the meanings):
  
  if (any(colnames(this_tab) == 'clapping')) {
  this_tab <- this_tab[, -which(colnames(this_tab) == 'clapping')]
  }
  
  # Transform into distance matrix object (needed for Mantel test):
  
  this_tab <- as.dist(this_tab)
  
  # Save this in an object with the name pattern "EN_tab" etc.:
  
  assign(str_c(languages[i], '_tab'), this_tab)
}
```

Loop through languages and put this information into the corresponding matrices:

```{r, warning = FALSE, message = FALSE}
for (i in seq_along(languages)) {
  lang1 <- languages[i]
  for (j in seq_along(languages)) {
    lang2 <- languages[j]
    if (lang1 != lang2) {
      # Retrieve previously stored distance matrices:
      
      tab1 <- get(str_c(languages[i], '_tab'))
      tab2 <- get(str_c(languages[j], '_tab'))
      
      # Perform correlation:
      
      this_mantel <- mantel.rtest(tab1, tab2, nrepet = 999)
      
      # Store correlation value and p-value:
      
      r_vals[i, j] <- this_mantel$obs
      p_vals[i, j] <- this_mantel$pvalue
      
    }
  }
}
```

Check results:

```{r}
## Correlation values:

round(r_vals, 2)
```

What's the minimum and maximum correlation value?

```{r}
range(r_vals, na.rm = TRUE)
```

Which are those?

```{r}
row.names(r_vals)[r_vals %in% range(r_vals, na.rm = TRUE)]
```


What's the mean correlation?

```{r}
mean(r_vals, na.rm = TRUE)
```

What are the p-values?:

```{r}
p_vals
```

For how many comparisons would we have to correct?

```{r}
length(languages) * (length(languages) - 1) / 2
```

300 comparisons! That's a bit ridiculous:

```{r}
# Vector of adjusted p-values from upper triangle of the matrix:

adjusted_p <- p.adjust(p_vals[upper.tri(p_vals)], method = 'bonferroni', n = 300)

# How many significant?

sum(adjusted_p < 0.05)
```

None after correction.

The better approach would be to calculate how many we expect to be significant based on chance alone for an alpha level of 0.05.

```{r}
300 * 0.05
```

Only 15. And how many are significant? (before correction)

```{r}
sum(p_vals[upper.tri(p_vals)] < 0.05)
```

300 are significant!

This completes this analysis.




